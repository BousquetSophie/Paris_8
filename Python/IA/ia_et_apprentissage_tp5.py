# -*- coding: utf-8 -*-
"""IA_et_apprentissage_TP5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g0fuyY08arL-pwK5xrlh0FN9sM3TDetx

# Perceptron Multi-couches et reconnaissance de chiffres écrits à la main

Bienvenu dans ce TP d'IA et apprentissage, dans lequel nous allons implémenter votre premier modèle d'apprentissage profond!

De manière similaire à ce que nous avons fait la semaine dernière, nous allons nous intéresser à la tâche de reconnaissance automatique de chiffres écrits à la main, en utilisant le jeu de données MNIST (Lixed National Institute of Standards and Technology).

Ce jeu de données se décompose en un jeu d'entrainement et un jeu de test, qui contiennent tous les deux des images de chiffres écrits à la main, de 28x28 pixels en nuance de gris.

Utilisé comme un test standard de méthodes d'apprentissage dans les années 90 et 2000, ce jeu de données est aujourd'hui considéré "résolu" depuis un certain temps. Il reste en revanche un très utile pour apprendre à ajuster ses premiers modèles d'apprentissage profond, et nous servira dans ce TP et le suivant.

L'objectif de ce TP est d'ajuster un perceptron multicouche sur ce jeu de données et d'évaluer ses performances sur le jeu de test. Pour ce faire, nous utiliserons d'abord la librairie pytorch, que vous connaissez, puis la librairie Huggingface, librairie spécialisée en apprentissage profond (et tout particulièrement en traiterment automatique des langues), qui implémente un certain nombre de fonctionnalité qui permette de simplifier le développement d'un modèle d'apprentissage profond avec pytorch.

Commencez par executer la case suivante, qui importe toutes les librairies nécessaires à l'exécution du TP.
"""

!pip install Datasets
!pip install transformers

import torch
from torchvision.datasets import MNIST
from datasets import Dataset
from transformers import Trainer, TrainingArguments
import matplotlib
import matplotlib.pyplot as plt

"""# 1. Chargement, prétraitement et visualisation du jeu de données

Exécutez la cellule de code suivante, qui charge le jeu de données MNIST directement dans la session jupyter.
"""

dataset = MNIST(root='.', download=True, train=True)
test_dataset = MNIST(root='.', download=True, train=False)

# Convert the data and targets to numpy arrays for easier manipulation
images, labels = dataset.data.numpy(), dataset.targets.numpy()
test_images, test_labels = test_dataset.data.numpy(), test_dataset.targets.numpy()

# Prepare the data in a dictionary format
data = {'inputs': images.reshape([images.shape[0], -1]), 'labels': labels}
test_data = {'inputs': test_images.reshape([test_images.shape[0], -1]), 'labels': test_labels}

"""Comme d'habitude, commencez par vous familiarisez avec le jeu de données, en visualisant les données d'entrées, en vous intéressant au nombre d'observations présentes dans le jeu de données, comment les variables explicatives peuvent varier, et caetera."""

plt.matshow(images[2545])
plt.matshow(images[5338])
plt.matshow(images[3181])
plt.show()
plt.hist(labels)

"""Que pouvez vous dire de ce jeu de données? Tout particulièrement, que constatez vous vis à vis du nombre d'observation présentes dans ce jeu de données, en comparaison à ceux que vous avez rencontré en cours jusqu'ici?

En apprentissage profond (en en apprentissage machine en général), il est typiquement conseillé de "normaliser" ses variables prédictives avant de les utiliser pour ajuster un modèle. Plusieurs méthodes de normalisations sont envisageables en fonction des contextes, mais il est typique en analyse d'image de faire en sorte que les valeurs de pixels soient comprises entre 0 et 1. Transformez vos variables explicatives (c'est à dire les array images et test_images) afin de faire en sortes que leurs valeurs minimales et maximales valent 0 et 1 respectivement.
"""

import numpy as np
images = images.astype(np.float32)
test_images = test_images.astype(np.float32)
images /= images.astype(np.float32).max()
test_images /= test_images.astype(np.float32).max()

"""# 2.Ajustement d'u modèle avec Pytorch

Maintenant que vous avez préparé votre jeu de données, il est temps d'ajuster un modèle d'apprentissage profond avec PyTorch!

# 2.1 Préparation des données pour l'ajustement du modèle

Comme vous l'avez vu, le nombre d'observation présentes dans le jeu de données d'entraînement est considérable. Ceci risque de nous poser des problèmes de temps et de mémoire, si l'on ajuste nos modèles par descente de gradient directement, comme nous l'avons toujours fait.

Pour pallier à cette problématique d'ordre purement calculatoire, les modèles d'apprentissage profond sont typiquement ajustés par descente de gradient stochastique, une variante de la descente de gradient qui fonctionne comme suit:

*   A chaque itération de la descente de gradient, tirer au sort sans remise un certain nombre d'observation du jeu de données
*   Calculer les prédictions du modèle pour chacune de ces observations sélectionnées
*   Calculer la fonction objectif relative à ces observations
*   Calculer le gradient de cette fonction objectif et mettre à jours les paramètres
*   Continuer jusqu'à ce que toutes les observations du jeu de données ont été tirée au sort, puis recommencer

Le nombre d'observations tirées au sort à chaque itération est communément appelé "batch size" (taille de fournée), et utiliser une batch size petite (typiquement entre 32 et 64) permet de limiter les besoin en mémoires et en temps pour les calculs d'une itération de descente de gradient.

Cette étape supplémentaire, en revanche, nécessite que nous adaptions notre code pytorch. Heureusement, pyTorch propose déjà un objet pour gérer toute cette complexité supplémentaire, l'objet DataLoader.

Un objet Dataloader peut être instancié à partir d'un Dataset Pytorhc, et peut s'utiliser comme un Iterable python, qui rend à chaque itération un groupe d'observations du jeu de données, de taille batch_size stipulée dans le constructeur du dataloader.

La cellule suivante vous donne un exemple d'instanciation d'un dataset et d'un dataloader pytorch à partir des array numpy de notre jeu de données.





"""

torch_train_dataset = torch.utils.data.TensorDataset(torch.tensor(data['inputs'], dtype=torch.float32), torch.tensor(data['labels']))
train_dataloader = torch.utils.data.DataLoader(torch_train_dataset, batch_size=64, shuffle=True)

"""# 2.2 Définition du modèle

Maintenant que nos données d'entraînement sont prêtes, il est tant de définir notre modèle!

Comme nous l'avons vu tout à l'heure, un perceptron multi-couche est défini comme une succession de transformation linéaires (définies par des multiplications matricielles), et de transformationss scalaires non linéaires, comme la fonction ReLU.

PyTorch propose un objet permettant de très rapidement instancier pareil modèle, l'objet Sequential.

Un objet Sequential s'instancie comme une successions de couches de neurones, que nous sommes libres de choisir comme bon nous semble. Pour ce TP, les couches torch.nn.Linear et torch.nn.Relu nous seront particulièrement utiles!

Il nous reste tout de même à définir la dimensionalité de chacune des couches de ce modèle, qui reste un choix heuristique, difficile à deviner à priori.

Instanciez votre modèle dans la cellule de code suivante :
"""

model = torch.nn.Sequential(
    torch.nn.Linear(28*28, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 100),
    torch.nn.ReLU(),
    torch.nn.Linear(100, 10)
)

"""Maintenant que le modèle est défini, il nous suffit de l'ajuster par descente de gradient, comme nous l'avons toujours fait!

Il ne nous reste plus qu'à finir d'implémenter la fonction train_model, en vous inspirant des derniers TPs!
"""

def train_model(dataloader, model, num_epochs=5, learning_rate=0.01):

    # On définit notre fonction perte
    objectif = torch.nn.CrossEntropyLoss()

    # On définit notre algorithme d'optimisation (SGD: Stochastic Gradient Descent)
    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)

    for epoch in range(num_epochs):

        for i, data in enumerate(dataloader, 0):

            inputs, labels = data

            optimizer.zero_grad()

            # Calculez les prédictions
            outputs = model(inputs)

            # Calculez la fonction objectif à partir des prédictions et des targets
            loss = objectif(outputs, labels)

            # Calculez le gradient
            optimizer.zero_grad()
            loss.backward()

            # Appliquez une itération de descente de gradient aux paramètres du modèle
            optimizer.step()

            if i % 100 == 0:
                print("epoch %d     step %d     loss = %.4f" % (epoch, i, loss))

    print('Finished Training')

    return None

"""Nous disposons maintenant de tous les éléments nécessaires à l'implémentation et l'ajustement du modèle.

Il ne nous reste plus qu'à ajuster, et évaluer les performances sur le jeu de données de test!
"""

train_model(train_dataloader, model)

"""# 3. Ajustement avec HuggingFace

Ajuster un modèle d'apprentissage profond avec pytorch directement est un peu laborieux. Ca ne se voit peut être pas dans notre exemple précédent, mais pour correctement ajuster des modèles modernes, tout une série de petites astuces sont nécessaires, et chacune amène une couche de complexité dans le programme.

La librairie HuggingFace consiste en une surcouche sur pytorch, initialement prévue pour les modèles de langue, qui implémente directement ces méthodes, et simplifie considérablement le processus d'ajustement d'un modèle.

Nous allons maintenant voir ensemble comment ajuster le même modèle avec cette librairie!
"""
