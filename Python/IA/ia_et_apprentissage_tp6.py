# -*- coding: utf-8 -*-
"""IA_et_apprentissage_TP6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxCujTxrIpTBtEuS7EaFg96futIUypcz

# Réseau à convolution et reconnaissance de chiffres écrits à la main

Bienvenu dans ce TP d'IA et apprentissage, dans lequel nous allons continuer de nous familiariser avec les méthodes d'apprentissage profond.

La semaine dernière, nous avions réussi à ajuster un perceptron multi-couche sur le jeu de données MNIST. Même si cette approche donnait de (relativement) bon résultats, avec une accuracy d'environ 97%, nous venons de voir en cours que les réseaux à convolution constituent une famille de modèles bien plus adaptés à l'analyse d'images (ou de données en structure de grille, plus généralement).

Rappelez vous, le jeu MNIST se décompose en un jeu d'entrainement et un jeu de test, qui contiennent tous les deux des images de chiffres écrits à la main, de 28x28 pixels en nuance de gris.

L'objectif de ce TP est d'ajuster un petit réseau de neurones à convolution sur ce jeu de données et d'évaluer ses performances sur le jeu de test. Pour ce faire, nous utiliserons d'abord le code pytorch que nous avons implémenté la semaine dernière la librairie pytorch, puis la librairie Huggingface, spécialisée en apprentissage profond (et tout particulièrement en traiterment automatique des langues), qui implémente un certain nombre de fonctionnalité qui permette de simplifier le développement d'un modèle d'apprentissage profond avec pytorch.

Commencez par executer les deux cases suivantes, qui importe toutes les librairies nécessaires à l'exécution du TP.
"""

!pip install transformers
!pip install datasets

!pip install accelerate -U

import torch
from torchvision.datasets import MNIST
import matplotlib
import matplotlib.pyplot as plt
from transformers import Trainer, TrainingArguments

"""# 1. Chargement, prétraitement et visualisation du jeu de données

Chargez le jeu de données MNIST directement dans la session jupyter. N'hésitez pas à vous inspirer du code présent dans le TP de la semaine dernière!

Attention en revanche, quand vous préparerez les dictionnaires contenant les données. La semaine dernière, nous avions applati les images afin de pouvoir les injecter dans un perceptron multicouche. Cette fois ci, puisque nous utilisons un réseau à convolution, les entrées doivent être des tenseurs 3D de shape [num_observations, profondeur, hauteur, largeur], qui est le format que les couches de convolution de Pytorch attendent.
"""

dataset = MNIST(root='.', download=True, train=True)
test_dataset = MNIST(root='.', download=True, train=False)

# Convert the data and targets to numpy arrays for easier manipulation
images, labels = dataset.data.numpy(), dataset.targets.numpy()
test_images, test_labels = test_dataset.data.numpy(), test_dataset.targets.numpy()

images = images / 255
test_images = test_images / 255

# Prepare the data in a dictionary format
data = {'inputs': images.reshape([-1, 1, 28, 28]), 'labels': labels}
test_data = {'inputs': test_images.reshape([-1, 1, 28, 28]), 'labels': test_labels}

"""Comme d'habitude, commencez par vous familiarisez avec le jeu de données, en visualisant les données d'entrées, en vous intéressant au nombre d'observations présentes dans le jeu de données, comment les variables explicatives peuvent varier, et caetera."""

plt.matshow(images[2545])
plt.matshow(images[5338])
plt.matshow(images[3181])
plt.show()
plt.hist(labels)

"""Comme dans le TP précédent, normalisez les images présentes dans vos deux dictionnaires de données. Il est très important que cette étape de prétraitement devienne seconde nature pour vous. En effet, elle ne coûte pas grand chose, et améliore facilite grandement la descente de gradient."""

import numpy as np
images = images.astype(np.float32)
test_images = test_images.astype(np.float32)
images /= images.astype(np.float32).max()
test_images /= test_images.astype(np.float32).max()

"""# 2.Ajustement d'u modèle avec Pytorch

Maintenant que vous avez préparé votre jeu de données, il est temps d'ajuster un modèle d'apprentissage profond avec PyTorch!

# 2.1 Préparation des données pour l'ajustement du modèle

L'avantage considérable que nous avons aujourd'hui par rapport à la semaine dernière, c'est que tout le code que vous avez déjà écrit, hormis l'instanciation du modèle (qui inévitablement sera différent) reste parfaitement applicable!

La seule différence dans votre code sera la façon dont vous instanciez le modèle, qui sera cette fois ci un réseau de neurones à convolution!

Pour commencez, instanciez votre DataLoader d'entraînement, afin de pouvoir effectuez votre descentde gradient de manière stochastique (encore une fois, n'hésitez surtout pas à vous inspirer du TP précédent).

"""

torch_train_dataset = torch.utils.data.TensorDataset(torch.tensor(data['inputs'], dtype=torch.float32), torch.tensor(data['labels']))
train_dataloader = torch.utils.data.DataLoader(torch_train_dataset, batch_size=64, shuffle=True)

"""# 2.2 Définition du modèle

Celui ci sera de nouveau basé sur un objet torch.nn.Sequential, mais cette fois ci contenant une alternance de couches torch.nn.Conv2d et torch.nn.Relu.

Une étape sera cependant nécessaire avant de conclure notre modèle par une couche linéaire avec une sortie de dimensionalité le nombre de classe à prédire dans notre problème. En effet, comme mentionné plus haut, une couche Linear de pytorch s'attend à recevoir des données vectorielles, et non sous forme de grille. Il va donc nous falloir applatir notre dernière carte d'activation avant de la passer dans notre couche Linear, ce que l'on peut faire très simplement avec la couche torch.nn.Flatten.

N'hésitez pas à consulter la documentation des objets torch.nn.Conv2D et torch.nn.Flatten pour bien comprendre comment les instancier correctement, et comment régler les hyperparamètres de la convolution (nombre de neurones, taille des sous-matrices, taille du décalage, padding, etc).

Pour éviter d'avoir à relancer tout le temps la cellule jupyter instanciant notre modèle, nous allons cette fois ci écrire une fonction python qui l'instanciera pour nous. Celle ci est définie pour l'instant avec un unique argument, hidden_size (censé définir le nombre de neurones par couches pour chaque couche du modèle), mais n'hésitez pas à être créatif, il est possible d'instancier un modèle totalement automatiquement, par exemple à l'aide d'un dictionnaire d'hyperparamètres!
"""

def create_model(hidden_size):
    model = torch.nn.Sequential(
        torch.nn.Conv2d(1, hidden_size, 3, padding='same'),
        torch.nn.GELU(),
        torch.nn.Conv2d(hidden_size, hidden_size, 5, stride=2),
        torch.nn.GELU(),
        torch.nn.Conv2d(hidden_size, hidden_size, 3, padding='same'),
        torch.nn.GELU(),
        torch.nn.Conv2d(hidden_size, hidden_size, 5, stride=3),
        torch.nn.GELU(),
        torch.nn.Conv2d(hidden_size, hidden_size, 3),
        torch.nn.Flatten(),
        torch.nn.Linear(hidden_size, 10))
    return model

my_model = create_model(50)

"""Maintenant que le modèle est instancié, il nous suffit de l'ajuster par descente de gradient, comme nous l'avons toujours fait!

Et comme promis, la fonction train_model que vous avez implémentéla semaine dernière fonctionnera tout autant pour le modèle de cette semaine qu'elle fonctionnait pour le perceptron multicouches de la semaine dernière!

Implémentez là dans la cellule suivante.
"""

def train_model(dataloader, model, num_epochs=5, learning_rate=0.001):

    # On définit notre fonction perte
    objectif = torch.nn.CrossEntropyLoss()

    # On définit notre algorithme d'optimisation (SGD: Stochastic Gradient Descent)
    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)

    for epoch in range(num_epochs):

        for i, data in enumerate(dataloader, 0):

            inputs, labels = data

            optimizer.zero_grad()

            # Calculez les prédictions
            outputs = model(inputs)

            # Calculez la fonction objectif à partir des prédictions et des targets
            loss = objectif(outputs, labels)

            # Calculez le gradient
            optimizer.zero_grad()
            loss.backward()

            # Appliquez une itération de descente de gradient aux paramètres du modèle
            optimizer.step()

            if i % 100 == 0:
                print("epoch %d     step %d     loss = %.4f" % (epoch, i, loss))

    print('Finished Training')

    return None

"""Nous disposons maintenant de tous les éléments nécessaires à l'implémentation et l'ajustement du modèle. Il ne nous reste plus qu'à ajuster, et évaluer les performances sur le jeu de donnéres de test!"""

train_model(train_dataloader, my_model)

"""# 3. Ajustement avec HuggingFace

Ajuster un modèle d'apprentissage profond avec pytorch directement est un peu laborieux. Ca ne se voit peut être pas dans notre exemple précédent, mais pour correctement ajuster des modèles modernes, tout une série de petites astuces sont nécessaires, et chacune amène une couche de complexité dans le programme.

La librairie HuggingFace consiste en une surcouche sur pytorch, initialement prévue pour les modèles de langue, qui implémente directement ces méthodes, et simplifie considérablement le processus d'ajustement d'un modèle.

Nous allons maintenant voir ensemble comment ajuster le même modèle avec cette librairie!

En plus de notre modèle, il est nécessaire d'instancier deux autres types objets avec HuggingFace:


*   Des objets Dataset (de la librairie datasets), qui permettront à HuggingFace de gérer efficacement nos jeux de données
*   Un objet Trainer, qui s'occupe de gérer toute la complexité de la boucle d'ajustement par descente de gradient

L'objet Trainer présente un certain nombre d'avantage par rapport à la simple boucle pythonique que nous avons précédemment codé pour ajuster notre modèle. Il permet automatiquement le déploiement sur GPU, possède des fonctionnalités de sauvegarde et de checkpointing des modèles, incorpore des astuces de manipulation dynamique du pas d'apprentissage, et permet notamment d'évaluer régulièrement les performances du modèle sur le jeu de test, tout cela pendant l'apprentissage!

# 3.1 Préparation des données

Commencons donc par instanciez nos objets Dataset. Nous en aurons besoin d'un pour le jeu d'entraînement et d'un pour le jeu de test.

Il existe plein de manière d'instancier (puis de modifier) un Dataset HuggingFace, n'hésitez pas à regarder la documentation!

En l'occurence, il est tout à fait possible d'instancier un tel objet à partir des dictionnaires data et test_data que nous avons déjà construit, par le biais du constructeur Dataset.from_dict!
"""

!pip install datasets

from datasets import Dataset

hf_train_dataset = Dataset.from_dict(data)
hf_test_dataset = Dataset.from_dict(test_data)

"""Maintenant que nos données sont prêtes, il ne nous reste plus qu'à instancier un modèle, et un objet HuggingFace Trainer. La classe Trainer est construite pour utiliser des modèles de langues (ce que nous verrons plus tard), et n'est donc pas telle quelle utilisable par notre modèle. En revanche, il nous est tout à fait possible de la modifier par héritage pour l'adapter à nos besoins.

Tout ce dont nous aurons besoin est d'instancier de nouvelles versions des méthodes compute_loss et prediction_step de cet objet!

La méthode compute_loss prend en entrée notre modèle, un dictionnaire de tenseur (dont les clés sont celles définies dans notre objet Dataset, et retourne typiquement un tuple composé de la valeur de la fonction objectif, et des prédictions du modèle.

La méthode prediction_step, utilisée pendant l'évaluation, accepte les mêmes entrées que la méthode compute loss, et renvoie un tuple compoq& de la valeur de la fonction objectif, les logits (log-probabilités, la sortie du modèle avant de la passer au softmax) prédits par le modèle, ainsi que les labels, afin de permettre le calcul de métriques.

Comme la méthode prediction_step ne s'utilise pas pendant la descente de gradient, il est **très fortement** conseillé de prévenir torch qu'il n'est pas nécessaire de le calculer durant l'exécution de cette méthode. Ceci économise considérablement les ressources en mémoire nécessaire au calcul, et peut se faire très simplement en écrivant toute la logique de la fonction dans un bloc indenté par "with torch.no_grad()".

Implémentez maintenant ces deux méthodes dans l'objet CustomTrainer, instanciez ci dessous!
"""

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        inputs = inputs.get("inputs")

        outputs = model(inputs.get("labels").view(-1))
        loss = torch.nn.CrossEntropyLoss()(outputs, labels)

        return (loss, outputs) if return_outputs else loss

    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):

        labels = inputs.get("labels")
        inputs = inputs.get("inputs")

        with torch.no_grad():
            labels = inputs.get("labels").view(-1)
            logits = model(inputs.get("inputs"))
            loss = torch.nn.CrossEntropyLoss()(logits, labels)

        return (loss, logits, labels)

"""Maintenant que notre classe CustomTrainer est implémentée, il ne nous reste plus qu'à l'instancier!

Le constructeur d'un objet Trainer requiert une instance d'un autre objet HuggingFace, un objet TrainingArguments.

Cet objet est un conteneur de données, qui permet de régler tout un tas de détails qui influenceront le comportement de votre objet Trainer. N'hésitez surtout pas à consulter la documentation de cet objet, il est particulièrement important de bien la maîtriser pour ne pas faire de bêtise quand on utilise HuggingFace (qui est une librairie qui aide beaucoup, mais qui cache également beaucoup).

Voici quelques arguments particulièrement important:


*   output_dir: Le seul argument nécessaire à l'instanciation. Le repertoire dans lequel 'le trainer pourra sauvegarder et stocker des données pendant l'ajustement. C'est notamment dans ce répertoire que votre Trainer sauvegardera votre modèle, si vous lui demander de le faire
*   learning_rate: Le pas d'apprentissage de votre algorithme de descente (choisi comme AdamW par défaut dans HuggingFace. C'est une variante de la descente de gradient stochastique classique, spécifiquement développée pour contrebalancer les difficultés d'ajustement d'un modèle d'apprentissage profond)
*   num_train_epochs: Le nombre de fois que le jeu de données d'entraînement sera parcouru pendant l'ajustement (et donc indirectement le nombre d'itérations de descente)
* per_device_train_batch_size: La taille de batch pendant l'entraînement
* per_device_eval_batch_size: La taille de batch pendant les évaluations
*   evaluation_strategy: Indique au Trainer si vous préférez évaluer votre modèle toutes les epochs, ou après un certain nombre d'itérations de descente
*   save_strategy: Indique si et quand sauvegarder votre modèle (on marquera "no" dans notre cas, puisque l'on ne cherche pas à sauvegarder nos modèles)
"""

training_args = TrainingArguments(
    output_dir = 'output_dir',
    learning_rate=.001,
    num_train_epochs=5,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=500,
    evaluation_strategy='steps',
    eval_steps=200,
    remove_unused_columns=False,
    logging_steps=10,
    save_strategy='no'
)

"""Dernière étape avant l'ajustement de notre modèle, l'instanciation de notre CustomTrainer!

Comme nous l'avons précédemment mentionné, les objets Trainer permettent d'évaluer régulièrement notre modèle pendant l'ajustement. Ceci présente deux avantages:


*   On peut monitorer l'ajustement de notre modèle, et vite l'arrêter si ca ne fonctionne pas
*   On peut arrêter d'ajuster le modèle quand celui commence à surrajuster. Cette technique est très utilisée en apprentissage profond, et est connue sous le nom d'early stopping (arrêt précoce)

De manière à indiquer au Trainer la ou les métriques que nous cherchons à calculer, celui ci accept un argument "compute_metrics", comme un callable (une fonction python), qui prend en entrée un objet HuggingFace EvalPredictions (un simple conteneur de données avec au moins deux attributs, predictions et label_ids, que l'on utilisera pour calculer nos métriques), et retourne un dictionnaire contenant notre (ou nos) métriques.

Comme la semaine dernière, nous nous intéressons à l'accuracy du modèle, c'est à dire son pourcentage de bonnes réponses.

Pour cette fois, je vous ai implémenté la fonction de calcul de métrique. Il ne vous reste plus qu'à instancier votre trainer, puis à ajuster le modèle en appelant la méthode train() du votre Trainer!


"""

def compute_metric_fn(eval_preds):
  predictions = eval_preds.predictions
  labels = p.label_ids

  accuracy = (predictions.argmax(-1) == labels).mean()

  return {"accuracy": accuracy}


trainer = CustomTrainer(
    model = my_model,
    args = training_args,
    train_dataset=hf_train_dataset,
    eval_dataset=hf_test_dataset,
    compute_metrics=compute_metric_fn
)

trainer.train()

"""Comparez les performances des modèles que vous avez ajusté avec HuggingFace et directement avec Pytorch. Que constatez vous?

(Ecrivez votre réponse dans cette cellule!)
"""
